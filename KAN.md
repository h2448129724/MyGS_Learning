# KAN(Kolmogorov-Arnold Networks)

## Kolmogorov-Arnold

柯尔莫哥洛夫-阿诺德定理是一个关于多变量函数的表示的定理，该定理阐述了如何将多元函数表示为一些单变量函数和加法、乘法运算的组合。

![image-20240528100713655](D:\研究生\科研\KAN.assets\image-20240528100713655-17168622368251.png)

## 柯尔莫哥洛夫-阿诺德定理详解


柯尔莫哥洛夫-阿诺德定理是关于多变量函数的一种表示方法。该定理说明，对于任何连续的实值函数$f$ （定义在 $\mathbb{R}^n$ 上），存在某个整数 $k$（取决于 $n$ 的值），以及一系列从 $\mathbb{R}$ 到 $\mathbb{R}$ 的连续单变量函数 $\phi_{ij}$ 和 $\psi_i$。使用这些函数，可以将 $f$ 表示为以下形式：




$$
f(x_1, x_2, \dots, x_n) = \sum_{i=1}^k \psi_i\left(\sum_{j=1}^n \phi_{ij}(x_j)\right)
$$

这里的 $\phi_{ij}$ 和 $\psi_i$ 是通过柯尔莫哥洛夫的特定构造方法选择的函数。

一组变量的连续有界函数的近似可以简化为求一维函数的多项式数：
$$
f(\mathbf{x}) = f(x_1, \ldots, x_n) = \sum_{q=1}^{2n+1} \Phi_q \left( \sum_{p=1}^n \phi_{q,p}(x_p) \right)
$$

$$
f(\mathbf{x}) = \sum_{i_{L-1}=1}^{n_{L-1}} \Phi_{L,i_L,i_{L-1}} \left( \sum_{i_{L-2}=1}^{n_{L-2}} \cdots \sum_{i_2=1}^{n_2} \left( \sum_{i_1=1}^{n_1} \Phi_{1,i_2,i_1} \left( \sum_{i_0=1}^{n_0} \Phi_{0,i_1,i_0}(x_{i_0}) \right) \right) \right),
$$



![img](https://pic3.zhimg.com/80/v2-9b17c5cafdfd6a6ab3aa052b41a08ac2_1440w.webp)

我们在 KAN 中寻找的不是任何函数，而是用样条函数对其进行参数化。

样条函数是一种光滑的曲线，它在不同区间上是由多项式定义的。每个样条函数都可以使用指定数量的控制点来近似。点数越多，逼近得就越精确。

样条函数是连续可微的，这意味着可以使用我们熟悉的反向传播误差方法来训练这种架构。

### 样条函数的基本形式

一个简单的样条函数例子是三次样条插值。假设你有一组数据点$ ((x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n))$，你希望找到一个函数$ ( S(x) )$，这个函数在每两个数据点之间用一个三次多项式插值，并且在所有数据点处函数值相等，即 $( S(x_i) = y_i )$。

三次样条函数的每个段可以写为：
$[ S_i(x) = a_i + b_i(x - x_i) + c_i(x - x_i)^2 + d_i(x - x_i)^3 ]$
其中，$( i )$ 表示样条的第 $( i )$ 段，参数 $( a_i, b_i, c_i, d_i )$ 需要根据数据点和光滑条件（比如连续性要求）来确定。

### 具体步骤如下：

1. **定义多项式**：在每个区间 $([x_i, x_{i+1}])$ 上定义一个三次多项式。
2. **满足插值条件**：确保 $( S(x_i) = y_i )$ 对所有数据点成立。
3. **连续性条件**：确保在接点 $( x_i )$ 的位置，函数的值、一阶导数和二阶导数都连续。



以下是区分KAN和感知器的关键方面：

- ###### 由于每个样条曲线的逼近需要多个点（假设我们有K个这样的点），KAN需要比具有相同深度和每层神经元数量的MLP多K倍的参数。
- 幸运的是，第一个问题可以通过KAN需要更少的神经元来弥补，以达到与MLP相同的准确性。研究人员通过实验证明，KAN在数据的泛化能力方面要好得多。
- 由于我们在KAN中训练的是函数而不是数字（参数），可以提高网络的准确性而无需从头开始重新训练。在MLP中，为了提高准确性，我们可以增加层数和神经元的数量，但这需要完全重新训练，并且并非总是有效。在KAN中，只需在逼近网格中添加更多的点就足够了。这保证了更好的结果，而无需重新训练神经网络。
- KAN比MLP更具可解释性。而解释性正是现代神经网络面临的主要问题之一。
- KAN在逼近复杂数学函数方面表现更好，因此可以说它具有更强的"技术智能"。文章中展示了KAN在解决微分方程方面比MLP好一个数量级，并且可以（重新）发现物理和数学定律。
- 这种架构有一个瓶颈：**KAN的学习速度大约比MLP慢10倍**。这可能会成为一个严重的障碍，但也有可能工程师们迅速学会优化这种网络的效率。

![img](https://pic3.zhimg.com/80/v2-5609b309f987d725e3573ac86b819b3a_1440w.webp)

![image-20240528104605276](D:\研究生\科研\KAN.assets\image-20240528104605276.png)

![image-20240528104809535](D:\研究生\科研\KAN.assets\image-20240528104809535.png)

#### 理论标量定律$（N^{-4}）$

在图3和图4中，黑色虚线代表了理论上的标量定律$ (N^{-4})$，这表示误差与参数数量$(N)$ 之间的关系遵循 $(\text{Error} \propto N^{-4})$ 的规律。这个标量定律表明，理论上，随着参数数量 $(N)$ 的增加，误差应该以 $(N^{-4})$ 的速度下降。

![image-20240528104054106](D:\研究生\科研\KAN.assets\image-20240528104054106.png)

