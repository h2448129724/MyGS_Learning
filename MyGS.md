# Gaussian Splatting for Reconstruction

## 研究思路/想法:

## 大致方向:

1. 使用Diffusion Model生成点云
2. 使用生成的点云通过Gaussian Splatting重建

## 问题:

1. 参考已经存在的Diffusion Model+NeRF进行三维重建，由于NeRF属于隐式重建，是否可以直接套用3D GS？

下面是一个可能的实施流程：

- 下面是一个可能的实施流程：

  步骤1: 使用Diffusion Model生成点云
  Diffusion Model：Diffusion models 在图像生成领域表现出色，且已经被扩展到生成三维数据（如点云）。首先，你需要训练一个diffusion model 来学习你的目标数据集（例如三维扫描或已有的三维模型集）的分布。
  生成点云：训练完成后，可以使用该模型生成新的点云数据。这些点云应代表场景的三维结构。
  步骤2: 使用3D Gaussian Splatting进行重建
  3D Gaussian Splatting：一旦拥有生成的点云，可以使用3D Gaussian Splatting技术来进行进一步的重建。在这个过程中，每个点云中的点都被视为具有一定半径的高斯核。
  体素化：将每个高斯核“splat”到一个体素网格上，通过累积贡献和融合重叠区域，可以构建出一个连续的体积表示。
  步骤3: 渲染和优化
  渲染：使用体积渲染技术可以将上述体积数据转换为可视化图像，这对于评估模型质量和进行进一步的分析很有用。
  优化：可能需要调整点的高斯权重、大小和分布，以优化最终重建的质量和真实感。

2. NeRF的隐式重建可以获得模型吗，还是只能获得某个视角下观察到的图片？

- **生成图片**：NeRF的主要用途是生成从特定视角观察到的图片。通过在训练过程中调整网络，使其学习场景的体积密度和颜色分布，NeRF可以从新的、未见过的视角渲染出逼真的图片。
- **获取三维模型**：尽管NeRF本身是一种隐式表达，它并不直接输出传统意义上的三维模型（如网格或点云）。然而，通过对整个空间的密集采样和渲染，理论上可以从NeRF的表示中提取出三维数据。例如，通过在空间中均匀采样点并计算这些点的密度，可以生成点云表示。此外，一些研究通过从NeRF渲染的多个视角进行三维重建，尝试恢复出更传统的三维模型形式。

3. 如何解决NeRF/GS渲染时需要的视角/相机参数问题
4. 使用Diffusion Model还算是重建吗？
4. 点云补全/新视角合成/扩散模型生成点云之间有什么区别？

## 下周工作计划:

1. 解决问题1-5
2. 查看论文Diffusion Model+NeRF











## 基于点的渲染

![image-20240506204853103](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20240506204853103.png)



## 3D Reconstruction

### 3D Gaussian Splatting for Real-Time Radiance Field Rendering

![image-20240515144427600](D:\研究生\科研\MyGS.assets\image-20240515144427600.png)

input: a set of images of a static scene, together with the corresponding cameras calibrated by SfM

![image-20240512211514035](D:\研究生\科研\MyGS.assets\image-20240512211514035.png)



### FSGS: Real-Time Few-shot View Synthesis using Gaussian Splatting

explore reconstructing 3D scenes from sparse view input. It initializes sparse Gaussians from structurefrom-motion (SfM) methods and identifies them by unpooling existing Gaussians. To allow faithful geometry reconstruction, an extra pre-trained 2D depth estimation network helps to supervise the rendered depth images.

### SparseGS: Real-Time 360°Sparse View Synthesis using Gaussian Splatting

### CoherentGS: Sparse Novel View Synthesis with Coherent 3D Gaussians

### DNGaussian: Optimizing Sparse-View 3D Gaussian Radiance Fields with Global-Local Depth Normalization

target 3D reconstruction from sparse-view inputs by introducing depth inputs estimated by pre-trained a 2D network.

### GaussainObject: Just Taking Four Images to Get A High-Quality 3D Object with Gaussian Splatting

d initializes Gaussians with visual hull and fine-tunes a pre-trained ControlNet [99] repair degraded rendered images generated by adding noises to Gaussians' attributes, which outperforms previous NeRF-based sparse-view reconstruction methods as shown in Fig. 6.

### pixelSplat: 3D Gaussian Splats from Image Pairs for Scalable Generalizable 3D Reconstruction

reconstructs 3D scenes from single-view input without any data priors.

### MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images

brings the cost volume representation into sparse view reconstruction, which is taken as input for the attributes prediction network for Gaussians.

### Splatter Image: Ultra-Fast Single-View 3D Reconstruction

works on single-view data but instead utilizes a U-Net [104] network to translate the input image into attributes on Gaussians

### COLMAP-Free 3D Gaussian Splatting



[https: //oasisyang.github.io/colmap-free-3dgs.]()

While neural rendering has led to impressive advances in scene reconstruction and novel view synthesis, it relies heavily on accurately pre-computed camera poses. To relax this constraint, multiple efforts have been made to train Neural Radiance Fields (NeRFs) without pre-processed camera poses. However, the implicit representations of NeRFs provide extra challenges to optimize the 3D structure and camera poses at the same time. On the other hand, the recently proposed 3D Gaussian Splatting provides new opportunities given its explicit point cloud representations. **This paper leverages both the explicit geometric representation and the continuity of the input video stream to perform novel view synthesis without any SfM preprocessing.** We process the input frames in a sequential manner and progressively grow the 3D Gaussians set by taking one input frame at a time, without the need to pre-compute the camera poses. Our method significantly improves over previous*This work was done while Yang Fu was a part-time intern at NVIDIA.approaches in view synthesis and camera pose estimation under large motion changes. Our project page is https: //oasisyang.github.io/colmap-free-3dgs.

![image-20240507093100558](D:\研究生\科研\MyGS.assets\image-20240507093100558.png)

Overview of proposed CF-3DGS. Our method takes a sequence of images as input to learn a set of 3D Gaussian that presents the input scene and jointly estimates the camera poses of the frames. We first introduce a local 3DGS to estimate the relative pose of two nearby frames by approximating the Gaussian transformation. Then, a global 3DGS is utilized to model the scene by progressively growing the set of 3D Gaussian as the camera moves.

#### 计算两帧图像之间的相对相机位姿

计算相邻两帧视频流之间的相对相机位姿是计算机视觉领域中的一个常见问题，特别是在视觉里程计和SLAM（Simultaneous Localization and Mapping）中。这个过程通常包括提取特征点、匹配特征点、估计运动和优化。下面是计算相对相机位姿的一般步骤：

#### 步骤1: 特征检测与匹配
1. **特征提取**：首先，在每一帧图像中提取特征点。常用的特征提取算法包括SIFT（尺度不变特征变换）、SURF（加速稳健特征）、ORB（Oriented FAST and Rotated BRIEF）等。
2. **特征匹配**：使用特征描述符在连续的帧之间找到匹配的特征点。可以使用BF匹配器（暴力匹配）或FLANN匹配器（快速最近邻搜索库）。

#### 步骤2: 估计运动（运动恢复结构）
1. **基本矩阵/本质矩阵计算**：使用匹配的特征点计算基本矩阵（如果只知道相机内参）或本质矩阵（如果知道相机内参）。这可以通过RANSAC（随机抽样一致性）算法来做，以排除错误匹配并获得一个鲁棒的估计。
2. **从本质矩阵恢复旋转和平移**：如果使用本质矩阵，可以直接从中恢复相对旋转和平移向量。如果使用基本矩阵，需要结合相机内参转换为本质矩阵。

#### 步骤3: 优化（可选）
- **Bundle Adjustment**：通过最小化重投影误差对所有观测到的特征点位置以及相机位置进行全局优化，可以改进位姿估计的精度。

#### 示例代码（使用OpenCV和Python）
以下是如何使用OpenCV库来估计两帧之间的相对相机位姿的简单示例：

```python
import numpy as np
import cv2

# 读取图像
img1 = cv2.imread('frame1.jpg', 0)  # 查询图像
img2 = cv2.imread('frame2.jpg', 0)  # 训练图像

# 初始化ORB检测器
orb = cv2.ORB_create()

# 找到关键点和描述符
kp1, des1 = orb.detectAndCompute(img1, None)
kp2, des2 = orb.detectAndCompute(img2, None)

# 创建匹配器并进行匹配
bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)
matches = bf.match(des1, des2)
matches = sorted(matches, key=lambda x:x.distance)

# 绘制前N个匹配
img3 = cv2.drawMatches(img1, kp1, img2, kp2, matches[:30], None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)

# 计算本质矩阵
points1 = np.zeros((len(matches), 2), dtype=np.float32)
points2 = np.zeros((len(matches), 2), dtype=np.float32)

for i, match in enumerate(matches):
    points1[i, :] = kp1[match.queryIdx].pt
    points2[i, :] = kp2[match.trainIdx].pt

# 相机内参
K = np.array([[focal_length, 0, img1.shape[1]/2],
              [0, focal_length, img1.shape[0]/2],
              [0, 0, 1]])

# 使用RANSAC计算本质矩阵
E, mask = cv2.findEssentialMat(points1, points2, K, cv2.RANSAC, 0.999, 1.

0)

# 从本质矩阵恢复位姿
_, R, t, mask = cv2.recoverPose(E, points1, points2, K)

print('旋转矩阵:', R)
print('平移向量:', t)

# 显示匹配结果
cv2.imshow('Matches', img3)
cv2.waitKey(0)
cv2.destroyAllWindows()
```

在这个示例中，你需要替换 `focal_length` 和图像路径，并确保图像正确加载。此代码段提供了从两个图像帧中提取和匹配特征，计算本质矩阵，并从中恢复相机位姿的基础。

### iComMa: Inverting 3D Gaussians Splatting for Camera Pose Estimation via Comparing and Matching

### Unbounded Sparse-view Pose-free Gaussian Splatting in 40 Seconds

### Towards Pose-free Generalizable 3D Gaussian Splatting in Real-time



### GaussianObject: Just Taking Four Images to Get A High-Quality 3D Object with Gaussian Splatting



![image-20240511143237735](D:\研究生\科研\MyGS.assets\image-20240511143237735.png)



![image-20240511143252500](D:\研究生\科研\MyGS.assets\image-20240511143252500.png)

输入： 𝑁 reference images + camera parameters

Initial Optimization with Structure Priors

Gaussian Repair Model Setup

Gaussian Repair with Distance-Aware Sampling

给定一组稀疏的 𝑁 参考图像 𝑋ref = {𝑥𝑖 }𝑁 𝑖=1，在 360◦ 范围内捕获并包含一个对象，并且对应的相机参数 Πref = {𝜋𝑖 }𝑁 𝑖=1 和对象掩模 𝑀ref = {𝑚𝑖 }𝑁 𝑖=1，整体目标是获得一个 3D 表示 G，该表示可以从任何视点 𝑥 = G(𝜋 |{𝑥𝑖 , 𝜋𝑖 ,𝑚𝑖 }𝑁 𝑖=1) 实现照片般逼真的渲染。

GaussianObject ( a )通过使用相机参数和掩膜图像构建视觉外壳，使用Lgs对其进行优化，并通过浮点数消除对其进行精炼来初始化3D高斯。( b )使用了一种新颖的"留一法"策略，并在高斯图中添加3D噪声来生成损坏的高斯效果图。这些渲染图与对应的参考图像配对，便于使用Ltune训练高斯修复模型。( c )经过训练，冻结高斯修复模型，并利用该模型对需要校正的视图进行校正。这些观点通过距离敏感抽样进行识别。利用修复后的图像和参考图像，使用Lrep和Lgs进一步优化3D高斯。 作者：3D视觉工坊 https://www.bilibili.com/read/cv32010244/ 出处：bilibili

### DiffPoint: Single and Multi-view Point Cloud Reconstruction with ViT Based Diffusion Model

本文提出了一种名为DiffPoint的方法，用于点云重建。DiffPoint是一种基于ViT（Vision Transformers）的扩散模型，通过将时间、图像嵌入和噪点补丁等所有输入都视为标记，并在相同的特征空间内进行交互，实现了对点云的重建。DiffPoint的主要步骤包括噪点的初始处理、使用预训练的CLIP和特征聚合模块处理输入图像，以及使用标准的ViT骨干网络预测真实数据。为了解决点云的离散性，作者采用了Farthest Point Sampling（FPS）和K-Nearest Neighborhood（KNN）算法将点云划分为不规则的补丁，并使用名为PointNet的轻量级编码器将这些补丁编码为标记嵌入。实验结果表明，DiffPoint在单视图和多视图的3D重建任务中表现出色，并在CD（Chamfer Distance）和F-score指标上优于其他方法。





![image-20240513173155345](D:\研究生\科研\MyGS.assets\image-20240513173155345.png)

### PC2: Projection-Conditioned Point Cloud Diffusion for Single-Image 3D Reconstruction

![image-20240514120835851](D:\研究生\科研\MyGS.assets\image-20240514120835851.png)

![image-20240514120847414](D:\研究生\科研\MyGS.assets\image-20240514120847414.png)

### 3DFIRES: Few Image 3D REconstruction for Scenes with Hidden Surfaces

predict an accurate and consistent 3D reconstruction from one or more sparsely spaced camera views and known poses.

![image-20240516120220120](D:\研究生\科研\MyGS.assets\image-20240516120220120.png)



### Triplane Meets Gaussian Splatting: Fast and Generalizable Single-View 3D Reconstruction with Transformers

![image-20240521093537144](D:\研究生\科研\MyGS.assets\image-20240521093537144.png)![image-20240521093554000](D:\研究生\科研\MyGS.assets\image-20240521093554000.png)

Hybrid Triplane-Gaussian

3D Gaussian Decoder

Rendering

Reconstruction from Single-View Images



### AGG

在第一阶段，我们采用混合生成器，以粗分辨率生成 3D 高斯。分解几何和纹理的混合表示生成粗略的高斯预测。通过两个独立的transformer预测几何和纹理信息，可以联合优化三维高斯属性。

DINOv2（自监督的Transformer）

在这个阶段，我们将几何和纹理生成任务分解为两个不同的网络。几何transformer 解码从预先训练的图像特征提取器中提取的**图像特征**，并预测3D高斯的位置。另一个纹理转换器类似地生成一个纹理字段，稍后由高斯位置查询以获得其他点属性。

在第二阶段基于unet的具有点体素层的架构，我们利用点体素卷积网络来有效地提取局部特征，并从前一阶段对粗3D高斯进行超分辨。
3.2 Coarse Hybrid Generator
使用vision Transformer对输入图像编码，随后，构造一个几何和纹理生成器，将可学习的查询分别映射到位置序列和纹理场。然后将这种混合表示解码为显式的3D高斯，从而实现高效的高分辨率渲染，并通过多视图图像促进监督。
**Encoding Input RGB Information**. 以前的方法用CLIP，但是这毕竟不是用来提特征的，所以用预训练的DINOV2。

Geometry Predictor. location信息是通过3D均值向量来的，3维向量。采用了一个基于Transformer的网络。Transformer输入是一组可学习的查询，由一组可学习的位置嵌入实现。在输入到变压器网络之前，位置嵌入与从DINOv2模型中提取的全局标记[CLS]相加。

Transformer架构遵循DINOV2, 还用了cross attention.

Texture Field Generator. **预测纹理一个主要的挑战是在 3D 空间中缺乏对纹理的直接基本事实监督**。相反，纹理信息是通过2D中的渲染损失来推断的。

为了解决这个问题，使用不同的Transformer来生成纹理场。我们的纹理场是使用三平面[5]实现的，辅以共享解码MLP头。三平面接受几何分支的3D位置查询，输出纹理。利用该纹理场有助于分解几何和纹理信息的优化，并且不正确的几何预测不影响纹理分支的优化过程。

3.3. Gaussian Super Resolution
主要关注于细化局部细节。为简单起见，我们使用具有有效点体素层的轻量级 UNet 架构 。

Latent Space Super-Resolution. 我们执行特征扩展作为直接扩展点数的代理。通过点体素卷积编码器，我们首先将阶段一个粗糙的3D高斯转换为紧凑的潜在特征

Incorporating RGB information. 由于振荡点位置和粗糙几何形状，纹理场可能会收敛到模糊的结果。我们将 RGB 特征引入 UNet 架构的瓶颈。具体来说，我们在特征扩展操作之前和之后采用交叉注意力层。



# Sparse

###  [CVPR '24] Splatter Image: Ultra-Fast Single-View 3D Reconstruction



![image-20240713182016935](D:\研究生\科研\MyGS.assets\image-20240713182016935.png)

![image-20240713182350744](D:\研究生\科研\MyGS.assets\image-20240713182350744.png)

![image-20240713182415510](D:\研究生\科研\MyGS.assets\image-20240713182415510.png)

## [CVPR '24] Recon3D: High Quality 3D Reconstruction from a Single Image Using Generated Back-View Explicit Priors

Time：takes about 3 hours and uses only one Tesla 32GB V100 GPU.



![image-20240713182648675](D:\研究生\科研\MyGS.assets\image-20240713182648675.png)

![image-20240713182905595](D:\研究生\科研\MyGS.assets\image-20240713182905595.png)

## FDGaussian: Fast Gaussian Splatting from Single Image via Geometric-aware Diffusion Model

## MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images

## CAT3D: Create Anything in 3D with Multi-View Diffusion Models

### DREAMGAUSSIAN: GENERATIVE GAUSSIAN SPLAT-TING FOR EFFICIENT 3D CONTENT CREATION

有代码



![image-20240715140344991](D:\研究生\科研\MyGS.assets\image-20240715140344991-17210234258421.png)

![image-20240715144710690](D:\研究生\科研\MyGS.assets\image-20240715144710690.png)


### [3D高斯泼溅还有哪些问题没有被解决好～ (qq.com)](https://mp.weixin.qq.com/s/ARrSU6f0yBzG9GIoOhCi9A)

1.在开放稍大的空间，单纯依靠手机，3D高斯泼溅场景重建的成功概率不高。

2.如果覆盖角度缺失，会在某些采集视角上，渲染的效果比较好，而在另外一些视角上表现会比较糟糕

3.采用在高处围绕建模对象旋转拍摄的方法，通常会取得比较好的重建效果，无规律的漫游拍摄整体效果会比较差，但是室内连续空间就很难具备这样的条件。



### [帝国理工学院最新 | 超越Scaffold-GS：基于结构感知的3D Gaussian Splatting新视角合成方法 (qq.com)](https://mp.weixin.qq.com/s/5naNV9-YqfBfRwEsbROtMw)





### [三维模型重建技术发展研究（三）-3DGaussianSplatting：连续三维场景实时渲染（上） (qq.com)](https://mp.weixin.qq.com/s/pNjWSoRTTcN-3cAk0ESrzw)



### [最新开源 | 又快又好的扩散模型助力3D高斯场景补全 (qq.com)](https://mp.weixin.qq.com/s/B-W7uyGDoR2NGwMI-KzVcQ)



### [SIGGRAPH 2024 | 无需相机位姿的稀疏视角合成新方法 (qq.com)](https://mp.weixin.qq.com/s/PnRYIqrYSslr9qqYieu3FQ)



### [最新综述！3D Gaussian Splatting (qq.com)](https://mp.weixin.qq.com/s/I-Jurw6SVFrPcML_1A73hA)



### [太稳啦！Gaussian Splatting杀入6D物体位姿估计！ (qq.com)](https://mp.weixin.qq.com/s/7fKlTSYo0jNnmWDgiiAi-A)



[不止3D高斯！最最最新综述一览最先进的3D重建技术 (qq.com)](https://mp.weixin.qq.com/s/qU-xqClCU5RDNxem7aW7ag)
